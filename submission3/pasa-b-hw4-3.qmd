---
title: "Homework 4 - Submission 3"
subtitle: "ECON 470"
author: "Baran Pasa"
execute:
  echo: false
format:
  pdf:
    output-file: "pasa-b-hwk4-3"
    output-exit: "pdf"
    code-fold: true
    highlight-style: github
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}

jupyter: python3

---

# Homework 4 
### [Link to Github]('https://github.com/BaranPasa2/homework4') 

## Summarizing the Data
#### 1.

All SNPs, 800-series plans, and perscription drug only plans where removed from the dataset so that only plans that offer Part C benefits were included. Then, a box-and-whisper plot showing the distribution of plan counts by county over time was created. An accompanying table was also created to better digest the data.

```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false     
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Load the data
df = pd.read_csv('/Users/baranpasa/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Emory/Junior Year/Junior Spring/ECON 470/ECON 470 Python /homework4/data/output/final_ma_data.csv', low_memory=False)

filtered_df = df.copy()

columns_present = []
for col in ['snp', 'planid', 'plan_type', 'partd']:
    columns_present.append(col in df.columns)

if 'snp' in df.columns:
    # Check the data type to decide how to filter
    if pd.api.types.is_numeric_dtype(df['snp']):
        filtered_df = filtered_df[filtered_df['snp'] == 0]
    else:
        # If it's not numeric, try to filter based on string values
        filtered_df = filtered_df[~filtered_df['snp'].astype(str).str.contains('1|Yes|Y|True', case=False, na=False)]
if 'planid' in df.columns:
    # Check if planid is numeric
    if pd.api.types.is_numeric_dtype(df['planid']):
        filtered_df = filtered_df[~((filtered_df['planid'] >= 800) & (filtered_df['planid'] <= 899))]
    else:
        # If it's not numeric, convert to string and check
        filtered_df = filtered_df[~filtered_df['planid'].astype(str).str.match(r'8\d{2}')]

# Step 4: Filter out prescription drug only plans
# This is more complex as we need to check multiple conditions
if all(col in df.columns for col in ['plan_type', 'partd']):
    # Assuming that Part D only plans are marked in plan_type or partd
    pdo_mask = (filtered_df['plan_type'].astype(str).str.contains('Prescription Drug|PDP', case=False, na=False))
    if pd.api.types.is_numeric_dtype(df['partd']):
        pdo_mask = pdo_mask | (filtered_df['partd'] == 1 & ~filtered_df['plan_type'].astype(str).str.contains('MA', case=False, na=False))
    
    filtered_df = filtered_df[~pdo_mask]
else:
    # Alternative approach: keep only MA, HMO, PPO, etc. plans
    if 'plan_type' in df.columns:
        ma_mask = filtered_df['plan_type'].astype(str).str.contains('MA|HMO|PPO|Private Fee|Cost Plan|Medical', case=False, na=False)
        filtered_df = filtered_df[ma_mask]

# Group by county and year to count plans
if all(col in filtered_df.columns for col in ['county', 'year']):
    county_year_plan_counts = filtered_df.groupby(['county', 'year']).size().reset_index(name='plan_count')
    county_year_plan_counts['plan_count'] = county_year_plan_counts['plan_count']
    # Set the style for a cleaner look
    sns.set_style("whitegrid")
    plt.figure(figsize=(12, 8))
    
    # Create a more polished box plot
    ax = sns.boxplot(
        x='year', 
        y='plan_count', 
        data=county_year_plan_counts,
        palette="Blues",
        width=0.6,
        fliersize=3,
        linewidth=1.5
    )
    
    # Add a more descriptive title and labels with better formatting
    plt.title('Distribution of Medicare Advantage Plans by County (2010-2015)', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Year', fontsize=14, labelpad=10)
    plt.ylabel('Number of Plans Per County', fontsize=14, labelpad=10)
    
    # Improve the tick formatting
    plt.xticks(rotation=0, fontsize=12)
    plt.yticks(fontsize=12)
    
    # Add a light grid only on the y-axis for better readability
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Add a more concise summary of statistics as a table below
    years = sorted(county_year_plan_counts['year'].unique())
    stats_data = []
    
    for year in years:
        year_data = county_year_plan_counts[county_year_plan_counts['year'] == year]['plan_count']
        stats_data.append([
            year,
            f"{year_data.median():.0f}",
            f"{year_data.mean():.1f}",
            f"{year_data.min():.0f}",
            f"{year_data.max():.0f}"
        ])
    
    # Create a nicely formatted table for the statistics
    table = plt.table(
        cellText=stats_data,
        colLabels=['Year', 'Median', 'Mean', 'Min', 'Max'],
        loc='bottom',
        bbox=[0.15, -0.38, 0.7, 0.2],  # [left, bottom, width, height]
        cellLoc='center'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.5)  # Adjust table size for better readability
    
    # Adjust layout to make room for the table
    plt.subplots_adjust(bottom=0.25)
    
    # Save the figure with higher quality
 #   plt.savefig('ma_plan_distribution_by_county.png', dpi=300, bbox_inches='tight')
    plt.show()
```

Looking at the table and graph table above, We can see that after 2010 the number of plans with Part C benefits offered in each country dropped significantly. The county with the highest number of Part C plans dropped from 906 in 2010 to 513 in 2011, and the median and mean dropped from 29 and 48.7 to 18 and 30.2 respectively. This tred continued after 2011, with the number of plans remaining significantly lower than in 2010. 

The number of Medicare Advantage plans per county appears generally sufficient. Median plan availability dropped from 29 to 15, while some counties still had hundreds of plans, suggesting a highly skewed distribution. While it seems sufficient, the wide variation points to potential inequality between rural and urban counties.
{{< pagebreak >}}

#### 2. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
years_to_analyze = [2010, 2012, 2015]
star_rating_data = []

for year in years_to_analyze:
    year_data = df[df['year'] == year]
    if 'Star_Rating' in year_data.columns:
        # Convert to numeric and drop NAs
        valid_ratings = pd.to_numeric(year_data['Star_Rating'], errors='coerce').dropna()

        if not valid_ratings.empty:
            rating_counts = valid_ratings.value_counts().sort_index()
            for rating, count in rating_counts.items():
                star_rating_data.append({
                    'Year': year,
                    'Rating': rating,
                    'Count': count
                })
        else:
            for rating in [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]:
                star_rating_data.append({
                    'Year': year,
                    'Rating': rating,
                    'Count': 0
                })

# Create DataFrame
star_df = pd.DataFrame(star_rating_data)

# Make sure 'Rating' is ordered for plotting
star_df['Rating'] = star_df['Rating'].astype(str)
star_df['Rating'] = pd.Categorical(star_df['Rating'],
                                   categories=[str(r) for r in sorted(star_df['Rating'].astype(float).unique())],
                                   ordered=True)

# Plot
plt.figure(figsize=(12, 8))
sns.barplot(x='Rating', y='Count', hue='Year', data=star_df)

plt.title('Distribution of Star Ratings by Year (2010, 2012, 2015)', fontsize=16)
plt.xlabel('Star Rating', fontsize=14)
plt.ylabel('Number of Plans', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

As seen above, there are is no star data for the 2012. This highlights difficulties of working with different datasets, the data is not always complete.

Comparing 2010 to 2015, we can see that star rating overall improved. There was an incredibly high number of plans rated 2.5 in 2010, a number which then fell drastically in 2015. In 2015, most plans had a rating of 4.0. It is also worth noting that there were no plans with ratings of 2.0 in 2015, and no plans with ratings of 5.0 in 2010. 
{{< pagebreak >}}

#### 3. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false   
plt.figure(figsize=(12, 8))
if 'ma_rate' in df.columns:
    # Ensure ma_rate is numeric and handle NaN values
    df['ma_rate'] = pd.to_numeric(df['ma_rate'], errors='coerce')
    
    # Group by year and calculate average benchmark (ignoring NaN values)
    benchmark_by_year = df.groupby('year')['ma_rate'].mean().reset_index()
    
    # Plot benchmark trends
    plt.plot(benchmark_by_year['year'], benchmark_by_year['ma_rate'], 
             marker='o', linestyle='-', linewidth=2)
    plt.title('Average Benchmark Payment (2010-2015)', fontsize=16)
    plt.xlabel('Year', fontsize=14)
    plt.ylabel('Average Benchmark Payment ($)', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Calculate the percentage increase from 2010 to 2015 if both values exist
    if 2010 in benchmark_by_year['year'].values and 2015 in benchmark_by_year['year'].values:
        start_value = benchmark_by_year[benchmark_by_year['year'] == 2010]['ma_rate'].values[0]
        end_value = benchmark_by_year[benchmark_by_year['year'] == 2015]['ma_rate'].values[0]
        percent_increase = ((end_value - start_value) / start_value) * 100
        
        # Add text showing percentage increase
        plt.text(2012, benchmark_by_year['ma_rate'].max() * 0.9, 
                f'Increase from 2010 to 2015: ${end_value - start_value:.2f} ({percent_increase:.1f}%)',
                bbox=dict(facecolor='white', alpha=0.8))
        
        # Step 1: Create a copy of the main table
        benchmark_table = benchmark_by_year.copy()
        benchmark_table['Benchmark Payment ($)'] = benchmark_table['ma_rate'].round(2)
        benchmark_table = benchmark_table[['year', 'Benchmark Payment ($)']]

        # Step 2: Add summary row for 2010–2015 change
        summary_row = pd.DataFrame([{
            'year': '2010–2015 Change',
            'Benchmark Payment ($)': f"${end_value - start_value:.2f} ({percent_increase:.1f}%)"
        }])

        # Step 3: Append and display
        benchmark_table = pd.concat([benchmark_table, summary_row], ignore_index=True)
        benchmark_table.rename(columns={'year': 'Year'}, inplace=True)
        benchmark_table

plt.tight_layout()
plt.savefig('benchmark_payment_trend.png', dpi=300)
plt.show()
```

The graph slowly rised until 2014 where it drastically fell. However, due to the scale of the graph, the changes in average benchmark payment are not that significant when considering how large they are. 

In order to assess the overall trend of average benchmark payments, broader years need to be analyzed. Further research should look to see if average benchmark payment continued to fall in 2016 and further into the present. 
{{< pagebreak >}}

#### 4.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false   
plt.figure(figsize=(12, 8))
if all(col in df.columns for col in ['avg_enrollment', 'avg_eligibles']):
    # Ensure data is numeric
    df['avg_enrollment'] = pd.to_numeric(df['avg_enrollment'], errors='coerce')
    df['avg_eligibles'] = pd.to_numeric(df['avg_eligibles'], errors='coerce')
    
    # Calculate MA share and handle division by zero
    df['ma_share'] = df['avg_enrollment'] / df['avg_eligibles'].replace(0, np.nan)
    
    # Filter out infinite values and NaNs
    df['ma_share'] = df['ma_share'].replace([np.inf, -np.inf], np.nan)
    
    # Group by year and calculate average MA share (ignoring NaN values)
    ma_share_by_year = df.groupby('year')['ma_share'].mean().reset_index()
    
    # Plot MA share trends
    plt.plot(ma_share_by_year['year'], ma_share_by_year['ma_share'] * 100, 
            marker='o', linestyle='-', linewidth=2, color='green')
    plt.title('Average Medicare Advantage Share (2010-2015)', fontsize=16)
    plt.xlabel('Year', fontsize=14)
    plt.ylabel('Average MA Share (%)', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Calculate correlation between MA share and benchmark payment if both columns have valid data
    if 'ma_rate' in df.columns:
        # Merge the two datasets
        combined_data = pd.merge(ma_share_by_year, benchmark_by_year, on='year')
        
        # Drop any rows with NaN values to ensure valid correlation calculation
        combined_data = combined_data.dropna(subset=['ma_share', 'ma_rate'])
        
        # Calculate correlation only if there are enough data points
        if len(combined_data) >= 2:
            correlation, p_value = pearsonr(combined_data['ma_share'], combined_data['ma_rate'])
            
            # Add text showing correlation coefficient
            plt.text(2012, ma_share_by_year['ma_share'].max() * 100 * 0.9, 
                    f'Correlation with benchmark: {correlation:.2f}',
                    bbox=dict(facecolor='white', alpha=0.8))
            
            # Calculate the percentage change from 2010 to 2015 if both values exist
            if 2010 in ma_share_by_year['year'].values and 2015 in ma_share_by_year['year'].values:
                start_share = ma_share_by_year[ma_share_by_year['year'] == 2010]['ma_share'].values[0] * 100
                end_share = ma_share_by_year[ma_share_by_year['year'] == 2015]['ma_share'].values[0] * 100
                share_change = end_share - start_share
                

plt.tight_layout()
plt.savefig('ma_share_trend.png', dpi=300)
plt.show()
```

The graph shows a steady increase in the average Medicare Advantage (MA) share from 2010 to 2015, rising from about 1.2% to nearly 1.9%. This trend suggests growing enrollment in MA plans over time. However, the low correlation with benchmark rates (0.04) implies that this growth was likely driven by factors other than payment incentives, such as plan quality or beneficiary preferences. A brief plateau around 2013 may reflect temporary market or policy adjustments.
{{< pagebreak >}}

## Estimating Average Treatment Effects (ATEs)
#### 5.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
#| results: asis
   
from pyfixest.estimation import feols

df_2010 = pd.read_csv('/Users/baranpasa/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Emory/Junior Year/Junior Spring/ECON 470/ECON 470 Python /homework4/data/output/final_ma_data.csv')

#set the dataset to only show year 2010
df_2010 = df_2010[df_2010['year'] == 2010].copy()
rating_columns = [
    'breastcancer_screen', 'rectalcancer_screen', 'cv_cholscreen', 'diabetes_cholscreen',
    'glaucoma_test', 'monitoring', 'flu_vaccine', 'pn_vaccine', 'physical_health',
    'mental_health', 'osteo_test', 'physical_monitor', 'primaryaccess',
    'hospital_followup', 'depression_followup', 'nodelays', 'carequickly',
    'overallrating_care', 'overallrating_plan', 'calltime',
    'doctor_communicate', 'customer_service', 'osteo_manage',
    'diabetes_eye', 'diabetes_kidney', 'diabetes_bloodsugar',
    'diabetes_chol', 'antidepressant', 'bloodpressure', 'ra_manage',
    'copd_test', 'betablocker', 'bladder', 'falling', 'appeals_timely',
    'appeals_review'
]

# Create new column with the row-wise average
df_2010 = df_2010.dropna(subset=['partc_score', 'avg_enrollment'])
df_2010['raw_rating'] = df_2010[rating_columns].mean(axis=1, skipna=True)

# Define valid ratings
# Define function to round to nearest 0.5
def round_half(x):
    return np.round(x * 2) / 2

# Apply the rounding
df_2010['rounded_rating'] = df_2010['raw_rating'].apply(round_half)

# Define valid ratings
valid_ratings = [3.0, 3.5, 4.0, 4.5, 5.0]

# Count and reindex
rating_counts = (
    df_2010
    .loc[df_2010['rounded_rating'].isin(valid_ratings), 'rounded_rating']
    .value_counts()
    .reindex(valid_ratings, fill_value=0)
    .sort_index()
)

# Format as a table
rating_table = rating_counts.reset_index()
rating_table.columns = ['Star Rating', 'Number of Plans']
rating_table  # This will render as a markdown table in Quarto

```

The table above shows the a breakdown of plans rounded up to the nearest half-star rating. Compared to 4.0 rated plans and below, there is a miniscule number, only 50 plans, that could be rounded to 4.5 stars. 

#### 6.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
def run_rd(df, cutoff, bandwidth):
    df = df.copy()
    df['treat'] = (df['raw_rating'] >= cutoff).astype(int)
    
    # Keep only observations within the bandwidth window
    df_band = df[(df['raw_rating'] >= cutoff - bandwidth) & (df['raw_rating'] <= cutoff + bandwidth)].copy()
    
    # Center the running variable
    df_band['running'] = df_band['raw_rating'] - cutoff

    # Run the RD regression
    model = feols("avg_enrollment ~ treat + running", data=df_band)
    
    return model

# Run RD at 3.0 and 3.5 cutoffs
model_3 = run_rd(df_2010, cutoff=3.0, bandwidth=0.125)
model_3_5 = run_rd(df_2010, cutoff=3.5, bandwidth=0.125)

# Extract results properly by calling the methods
coef_3 = model_3.coef()
se_3 = model_3.se()
pval_3 = model_3.pvalue()

coef_3_5 = model_3_5.coef()
se_3_5 = model_3_5.se()
pval_3_5 = model_3_5.pvalue()

# Summarize in a table
results = pd.DataFrame({
    'Cutoff': ['3.0 Stars', '3.5 Stars'],
    'Estimate (Treatment Effect)': [coef_3['treat'], coef_3_5['treat']],
    'Std. Error': [se_3['treat'], se_3_5['treat']],
    'p-Value': [pval_3['treat'], pval_3_5['treat']]
})

results_table = results
results_table.columns = ['Cutoff Estimate', '(Treatment Effect)', 'Std. Error', 'p-Value']
results_table
```
Neither the 3.0-star nor the 3.5-star cutoff shows a statistically significant effect on the outcome variable, as indicated by the high p-values (> 0.05). The negative effect at 3.0 stars is large in magnitude but imprecise, while the small positive effect at 3.5 stars is also not distinguishable from zero. These results suggest no clear evidence that barely receiving a higher star rating at these thresholds had a meaningful causal effect—though further analysis with larger samples or refined bandwidths may be useful.
{{< pagebreak >}}

#### 7. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
# Run RD at 3.0 and 3.5 cutoffs with different bandwidths
model_3_01 = run_rd(df_2010, cutoff=3.0, bandwidth=0.1)
model_3_012 = run_rd(df_2010, cutoff=3.0, bandwidth=0.12)
model_3_013 = run_rd(df_2010, cutoff=3.0, bandwidth=0.13)
model_3_014 = run_rd(df_2010, cutoff=3.0, bandwidth=0.14)
model_3_015 = run_rd(df_2010, cutoff=3.0, bandwidth=0.15)

model_35_01 = run_rd(df_2010, cutoff=3.5, bandwidth=0.1)
model_35_012 = run_rd(df_2010, cutoff=3.5, bandwidth=0.12)
model_35_013 = run_rd(df_2010, cutoff=3.5, bandwidth=0.13)
model_35_014 = run_rd(df_2010, cutoff=3.5, bandwidth=0.14)
model_35_015 = run_rd(df_2010, cutoff=3.5, bandwidth=0.15)


# Store all models with identifiers
models = {
    (3.0, 0.10): model_3_01,
    (3.0, 0.12): model_3_012,
    (3.0, 0.13): model_3_013,
    (3.0, 0.14): model_3_014,
    (3.0, 0.15): model_3_015,
    (3.5, 0.10): model_35_01,
    (3.5, 0.12): model_35_012,
    (3.5, 0.13): model_35_013,
    (3.5, 0.14): model_35_014,
    (3.5, 0.15): model_35_015,
}

# Collect results
rows = []
for (cutoff, bw), model in models.items():
    coef = model.coef()
    se = model.se()
    pval = model.pvalue()
    rows.append({
        'Cutoff': cutoff,
        'Bandwidth': bw,
        'Estimate': coef['treat'],
        'Std. Error': se['treat'],
        'p-Value': pval['treat']
    })

# Create DataFrame
results_df = pd.DataFrame(rows)

# Pivot into multi-level column table (like a grouped summary table)
pivot_df = pd.pivot_table(
    results_df,
    index='Bandwidth',
    columns='Cutoff',
    values=['Estimate', 'Std. Error', 'p-Value']
)

# Optional: Clean column names for readability
pivot_df.columns.names = ['Metric', 'Cutoff']
pivot_df
```
* No strong or consistent evidence that crossing the 3.0 or 3.5 star thresholds has a significant impact on the outcome (likely enrollment).

* The 3.0-star cutoff results show large negative estimates but high variability, undermining confidence.

* The one highly significant p-value (for 3.0 at 0.15) should be interpreted cautiously — it may reflect noise due to the wider bandwidth rather than a true effect.

#### 8.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
plt.figure(figsize=(12, 5))

# First plot: distribution around 3.0
plt.subplot(1, 2, 1)
sns.histplot(df_2010['raw_rating'], bins=50, kde=False, color='skyblue')
plt.axvline(3.0, color='red', linestyle='--', label='3.0 Cutoff')
plt.title('Distribution of Raw Ratings around 3.0')
plt.xlabel('Raw Rating')
plt.ylabel('Number of Plans')
plt.xlim(2.5, 3.5)
plt.legend()

# Second plot: distribution around 3.5
plt.subplot(1, 2, 2)
sns.histplot(df_2010['raw_rating'], bins=50, kde=False, color='lightgreen')
plt.axvline(3.5, color='red', linestyle='--', label='3.5 Cutoff')
plt.title('Distribution of Raw Ratings around 3.5')
plt.xlabel('Raw Rating')
plt.ylabel('Number of Plans')
plt.xlim(3.0, 4.0)
plt.legend()

plt.tight_layout()
plt.show()
```
These two histograms show the distribution of raw star ratings around the 3.0 and 3.5 cutoff points.

* At the 3.0 threshold, the distribution of the running variable shows potential signs of manipulation or rating bunching, which may challenge the validity of RD estimates there.
* In contrast, the 3.5 threshold appears more stable, suggesting it may be a more reliable setting for RD analysis.

##### Left Plot: 3.0 Cutoff
* There is a clear jump in density just below 3.0, especially around 2.7–2.9, with much higher counts of plans compared to the bins just above 3.0.
* The drop in frequency immediately after the cutoff suggests that fewer plans narrowly achieved a rating just above 3.0.
* This could indicate manipulation or bunching below the cutoff, where plans either failed to make it past the threshold or were rated conservatively.

##### Right Plot: 3.5 Cutoff
* The distribution appears smoother and more balanced around the 3.5 threshold.
* There's no obvious discontinuity or bunching just above or below 3.5 —> the frequency of plans seems fairly even across the window.
* This supports the RD assumption of continuity at 3.5, making it a more credible cutoff for causal inference.
{{< pagebreak >}}

#### 9.
```{python}
#| echo: false         
#| output: True      
#| warning: false     
#| message: false

# Step 1: Add binary indicators
df_2010['is_hmo'] = df_2010['plan_type'].str.contains('HMO', case=False, na=False).astype(int)
df_2010['has_partd'] = (df_2010['partd'] == 'Yes').astype(int)

# Step 2: Function to check balance
def covariate_balance(df, cutoff, bandwidth=0.125):
    df = df.copy()
    df_window = df[(df['raw_rating'] >= cutoff - bandwidth) & (df['raw_rating'] <= cutoff + bandwidth)].copy()
    df_window['above_cutoff'] = (df_window['raw_rating'] >= cutoff).astype(int)
    
    balance = df_window.groupby('above_cutoff')[['is_hmo', 'has_partd']].mean().T
    balance.columns = ['Below Cutoff', 'Above Cutoff']
    balance['Difference'] = balance['Above Cutoff'] - balance['Below Cutoff']
    return balance

# Step 3: Get balance tables
balance_3 = covariate_balance(df_2010, cutoff=3.0)
balance_35 = covariate_balance(df_2010, cutoff=3.5)

# Step 4: Combine and label
balance_3['Cutoff'] = '3.0'
balance_35['Cutoff'] = '3.5'
balance_combined = pd.concat([balance_3, balance_35])
balance_combined.reset_index(inplace=True)
balance_combined.rename(columns={'index': 'Characteristic'}, inplace=True)

# Round and return for Quarto to display as markdown table
balance_combined.round(3)
```



##### 10.

Based on the regression discontinuity results, there is no strong or consistent evidence that crossing the 3.0- or 3.5-star rating thresholds leads to a significant increase in plan enrollments. At the 3.0-star cutoff, the estimated treatment effect is –375.8, while at the 3.5-star cutoff it is 66.3. However, both estimates are statistically insignificant, with p-values of 0.23 and 0.65, respectively. The treatment effects are also not robust across different bandwidths, and the results vary considerably depending on the window used.

The histogram of raw ratings shows noticeable bunching just below the 3.0-star threshold, which suggests potential manipulation or rating inflation. This challenges the RD assumption of continuity around the cutoff and may bias the estimates. In contrast, the distribution around the 3.5-star threshold appears smoother, which is more supportive of a valid RD design.

Covariate balance tests further reveal meaningful imbalances near both thresholds. For example, plans above the 3.5-star cutoff are 29 percentage points more likely to be HMOs, and Part D coverage is lower above both thresholds. These differences indicate that plans just above and below the cutoff are not comparable in key dimensions, again challenging the RD design.

In conclusion, the results suggest that increasing a plan’s star rating from just below to just above a policy threshold does not lead to a statistically significant or robust increase in enrollments. Furthermore, violations of RD assumptions, particularly around the 3.0-star cutoff, limit the strength of causal claims.