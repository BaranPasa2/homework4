---
title: "Homework 4 - Submission 3"
subtitle: "ECON 470"
author: "Baran Pasa"
execute:
  echo: false
format:
  pdf:
    output-file: "pasa-b-hwk4-3"
    output-exit: "pdf"
    code-fold: true
    highlight-style: github
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}

jupyter: python3

---

# Homework 4 
### [Link to Github]('https://github.com/BaranPasa2/homework4') 

## Summarizing the Data
#### 1.

All SNPs, 800-series plans, and perscription drug only plans where removed from the dataset so that only plans that offer Part C benefits were included. Then, a box-and-whisper plot showing the distribution of plan counts by county over time was created. An accompanying table was also created to better digest the data.

```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false     
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Load the data
df = pd.read_csv('/Users/baranpasa/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Emory/Junior Year/Junior Spring/ECON 470/ECON 470 Python /homework4/data/output/final_ma_data.csv', low_memory=False)

filtered_df = df.copy()

columns_present = []
for col in ['snp', 'planid', 'plan_type', 'partd']:
    columns_present.append(col in df.columns)

if 'snp' in df.columns:
    # Check the data type to decide how to filter
    if pd.api.types.is_numeric_dtype(df['snp']):
        filtered_df = filtered_df[filtered_df['snp'] == 0]
    else:
        # If it's not numeric, try to filter based on string values
        filtered_df = filtered_df[~filtered_df['snp'].astype(str).str.contains('1|Yes|Y|True', case=False, na=False)]
if 'planid' in df.columns:
    # Check if planid is numeric
    if pd.api.types.is_numeric_dtype(df['planid']):
        filtered_df = filtered_df[~((filtered_df['planid'] >= 800) & (filtered_df['planid'] <= 899))]
    else:
        # If it's not numeric, convert to string and check
        filtered_df = filtered_df[~filtered_df['planid'].astype(str).str.match(r'8\d{2}')]

# Step 4: Filter out prescription drug only plans
# This is more complex as we need to check multiple conditions
if all(col in df.columns for col in ['plan_type', 'partd']):
    # Assuming that Part D only plans are marked in plan_type or partd
    pdo_mask = (filtered_df['plan_type'].astype(str).str.contains('Prescription Drug|PDP', case=False, na=False))
    if pd.api.types.is_numeric_dtype(df['partd']):
        pdo_mask = pdo_mask | (filtered_df['partd'] == 1 & ~filtered_df['plan_type'].astype(str).str.contains('MA', case=False, na=False))
    
    filtered_df = filtered_df[~pdo_mask]
else:
    # Alternative approach: keep only MA, HMO, PPO, etc. plans
    if 'plan_type' in df.columns:
        ma_mask = filtered_df['plan_type'].astype(str).str.contains('MA|HMO|PPO|Private Fee|Cost Plan|Medical', case=False, na=False)
        filtered_df = filtered_df[ma_mask]

# Group by county and year to count plans
if all(col in filtered_df.columns for col in ['county', 'year']):
    county_year_plan_counts = filtered_df.groupby(['county', 'year']).size().reset_index(name='plan_count')
    county_year_plan_counts['plan_count'] = county_year_plan_counts['plan_count']
    # Set the style for a cleaner look
    sns.set_style("whitegrid")
    plt.figure(figsize=(12, 8))
    
    # Create a more polished box plot
    ax = sns.boxplot(
        x='year', 
        y='plan_count', 
        data=county_year_plan_counts,
        palette="Blues",
        width=0.6,
        fliersize=3,
        linewidth=1.5
    )
    
    # Add a more descriptive title and labels with better formatting
    plt.title('Distribution of Medicare Advantage Plans by County (2010-2015)', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('Year', fontsize=14, labelpad=10)
    plt.ylabel('Number of Plans Per County', fontsize=14, labelpad=10)
    
    # Improve the tick formatting
    plt.xticks(rotation=0, fontsize=12)
    plt.yticks(fontsize=12)
    
    # Add a light grid only on the y-axis for better readability
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    
    # Add a more concise summary of statistics as a table below
    years = sorted(county_year_plan_counts['year'].unique())
    stats_data = []
    
    for year in years:
        year_data = county_year_plan_counts[county_year_plan_counts['year'] == year]['plan_count']
        stats_data.append([
            year,
            f"{year_data.median():.0f}",
            f"{year_data.mean():.1f}",
            f"{year_data.min():.0f}",
            f"{year_data.max():.0f}"
        ])
    
    # Create a nicely formatted table for the statistics
    table = plt.table(
        cellText=stats_data,
        colLabels=['Year', 'Median', 'Mean', 'Min', 'Max'],
        loc='bottom',
        bbox=[0.15, -0.38, 0.7, 0.2],  # [left, bottom, width, height]
        cellLoc='center'
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 1.5)  # Adjust table size for better readability
    
    # Adjust layout to make room for the table
    plt.subplots_adjust(bottom=0.25)
    
    # Save the figure with higher quality
 #   plt.savefig('ma_plan_distribution_by_county.png', dpi=300, bbox_inches='tight')
    plt.show()
```

Looking at the table and graph table above, We can see that after 2010 the number of plans with Part C benefits offered in each country dropped significantly. The county with the highest number of Part C plans dropped from 906 in 2010 to 513 in 2011, and the median and mean dropped from 29 and 48.7 to 18 and 30.2 respectively. This tred continued after 2011, with the number of plans remaining significantly lower than in 2010. 

The number of Medicare Advantage plans per county appears generally sufficient. Median plan availability dropped from 29 to 15, while some counties still had hundreds of plans, suggesting a highly skewed distribution. While it seems sufficient, the wide variation points to potential inequality between rural and urban counties.
{{< pagebreak >}}

#### 2. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
years_to_analyze = [2010, 2012, 2015]
star_rating_data = []

for year in years_to_analyze:
    year_data = df[df['year'] == year]
    if 'Star_Rating' in year_data.columns:
        # Convert to numeric and drop NAs
        valid_ratings = pd.to_numeric(year_data['Star_Rating'], errors='coerce').dropna()

        if not valid_ratings.empty:
            rating_counts = valid_ratings.value_counts().sort_index()
            for rating, count in rating_counts.items():
                star_rating_data.append({
                    'Year': year,
                    'Rating': rating,
                    'Count': count
                })
        else:
            for rating in [2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]:
                star_rating_data.append({
                    'Year': year,
                    'Rating': rating,
                    'Count': 0
                })

# Create DataFrame
star_df = pd.DataFrame(star_rating_data)

# Make sure 'Rating' is ordered for plotting
star_df['Rating'] = star_df['Rating'].astype(str)
star_df['Rating'] = pd.Categorical(star_df['Rating'],
                                   categories=[str(r) for r in sorted(star_df['Rating'].astype(float).unique())],
                                   ordered=True)

# Plot
plt.figure(figsize=(12, 8))
sns.barplot(x='Rating', y='Count', hue='Year', data=star_df)

plt.title('Distribution of Star Ratings by Year (2010, 2012, 2015)', fontsize=16)
plt.xlabel('Star Rating', fontsize=14)
plt.ylabel('Number of Plans', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```


Assessing the graph above, we can see a steady improvement in average star rating from 2010 to 2015. In 2010 the most common rating for a plan was 2.5. It then increased to 3.0 in 2012, and in 2015 the most popular star-rating was 4.0 by  significant amount. 2015 also saw no plans being rated as low as 2.0, and some plans were rated as high as 5.0. 
{{< pagebreak >}}

#### 3. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false   
plt.figure(figsize=(12, 8))
if 'ma_rate' in df.columns:
    # Ensure ma_rate is numeric and handle NaN values
    df['ma_rate'] = pd.to_numeric(df['ma_rate'], errors='coerce')
    
    # Group by year and calculate average benchmark (ignoring NaN values)
    benchmark_by_year = df.groupby('year')['ma_rate'].mean().reset_index()
    
    # Plot benchmark trends
    plt.plot(benchmark_by_year['year'], benchmark_by_year['ma_rate'], 
             marker='o', linestyle='-', linewidth=2)
    plt.title('Average Benchmark Payment (2010-2015)', fontsize=16)
    plt.xlabel('Year', fontsize=14)
    plt.ylabel('Average Benchmark Payment ($)', fontsize=14)
    plt.grid(True, linestyle='--', alpha=0.7)
    
    # Calculate the percentage increase from 2010 to 2015 if both values exist
    if 2010 in benchmark_by_year['year'].values and 2015 in benchmark_by_year['year'].values:
        start_value = benchmark_by_year[benchmark_by_year['year'] == 2010]['ma_rate'].values[0]
        end_value = benchmark_by_year[benchmark_by_year['year'] == 2015]['ma_rate'].values[0]
        percent_increase = ((end_value - start_value) / start_value) * 100
        
        # Add text showing percentage increase
        plt.text(2012, benchmark_by_year['ma_rate'].max() * 0.9, 
                f'Increase from 2010 to 2015: ${end_value - start_value:.2f} ({percent_increase:.1f}%)',
                bbox=dict(facecolor='white', alpha=0.8))
        
        # Step 1: Create a copy of the main table
        benchmark_table = benchmark_by_year.copy()
        benchmark_table['Benchmark Payment ($)'] = benchmark_table['ma_rate'].round(2)
        benchmark_table = benchmark_table[['year', 'Benchmark Payment ($)']]

        # Step 2: Add summary row for 2010–2015 change
        summary_row = pd.DataFrame([{
            'year': '2010–2015 Change',
            'Benchmark Payment ($)': f"${end_value - start_value:.2f} ({percent_increase:.1f}%)"
        }])

        # Step 3: Append and display
        benchmark_table = pd.concat([benchmark_table, summary_row], ignore_index=True)
        benchmark_table.rename(columns={'year': 'Year'}, inplace=True)
        benchmark_table

plt.tight_layout()
plt.savefig('benchmark_payment_trend.png', dpi=300)
plt.show()
```

The graph slowly rised until 2014 where it drastically fell. However, due to the scale of the graph, the changes in average benchmark payment are not that significant when considering how large they are. 

In order to assess the overall trend of average benchmark payments, broader years need to be analyzed. Further research should look to see if average benchmark payment continued to fall in 2016 and further into the present. 
{{< pagebreak >}}

#### 4.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false   
ma_share=df.copy()

ma_share['ma_share'] = ma_share['avg_enrolled'] / ma_share['avg_eligibles']
ma_share = ma_share.groupby('year')['ma_share'].mean().reset_index()

plt.figure(figsize=(10, 6))
plt.plot(ma_share['year'], ma_share['ma_share'], marker='o')
plt.title('Average Share of Medicare Advantage Over Time (2010-2015)')
plt.xlabel('Year')
plt.ylabel('Average Share of Medicare Advantage')
plt.grid(True)
plt.show()
```

The graph shows a steady increase in the average Medicare Advantage (MA) share from 2010 to 2015, rising from roughly 11% to over 30%. 

This growth shows the increase of private insurance participation in Medicare which should also result in decreasing government expenditure. However, when we look at average benchmark payments, we see that there has not been any significant decrease in prices. Further research into the payment structure of Medicare Advantage, as well as the data being used, is necessary in order to draw any conclusions.
{{< pagebreak >}}

## Estimating Average Treatment Effects (ATEs)
#### 5.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
#| results: asis
   

df_2010 = pd.read_csv('/Users/baranpasa/Library/Mobile Documents/com~apple~CloudDocs/Desktop/Emory/Junior Year/Junior Spring/ECON 470/ECON 470 Python /homework4/data/output/final_ma_data.csv')

#set the dataset to only show year 2010
df_2010 = df_2010[df_2010['year'] == 2010].copy()
rating_columns = [
    'breastcancer_screen', 'rectalcancer_screen', 'cv_cholscreen', 'diabetes_cholscreen',
    'glaucoma_test', 'monitoring', 'flu_vaccine', 'pn_vaccine', 'physical_health',
    'mental_health', 'osteo_test', 'physical_monitor', 'primaryaccess',
    'hospital_followup', 'depression_followup', 'nodelays', 'carequickly',
    'overallrating_care', 'overallrating_plan', 'calltime',
    'doctor_communicate', 'customer_service', 'osteo_manage',
    'diabetes_eye', 'diabetes_kidney', 'diabetes_bloodsugar',
    'diabetes_chol', 'antidepressant', 'bloodpressure', 'ra_manage',
    'copd_test', 'betablocker', 'bladder', 'falling', 'appeals_timely',
    'appeals_review'
]

# Create new column with the row-wise average
df_2010 = df_2010.dropna(subset=['partc_score', 'avg_enrollment'])
df_2010['raw_rating'] = df_2010[rating_columns].mean(axis=1, skipna=True)

#refine df_2010
core_col = [
    "contractid", "planid", "fips", "avg_enrollment", "state", "county", "partc_score", "avg_eligibles", "avg_enrolled",  "risk_ab", "Star_Rating",  "ma_rate", "plan_type", "partd", "raw_rating"
]
df_2010 = df_2010[core_col]

df_2010['market_share'] = df_2010['avg_enrolled'] / df_2010['avg_eligibles']
df_2010
# Define valid ratings
# Define function to round to nearest 0.5
def round_half(x):
    return np.round(x * 2) / 2

# Apply the rounding
df_2010['rounded_rating'] = df_2010['raw_rating'].apply(round_half)

# Define valid ratings
valid_ratings = [3.0, 3.5, 4.0, 4.5, 5.0]

# Count and reindex
rating_counts = (
    df_2010
    .loc[df_2010['rounded_rating'].isin(valid_ratings), 'rounded_rating']
    .value_counts()
    .reindex(valid_ratings, fill_value=0)
    .sort_index()
)

# Format as a table
rating_table = rating_counts.reset_index()
rating_table.columns = ['Star Rating', 'Number of Plans']
rating_table  # This will render as a markdown table in Quarto

```

Even when rounding plans up the nearest half-star rating, we can see that a majority of plans are below 3.0. There are very few plans above 4.0, and none above 4.5. This falls in line with the graphs shown earlier in popints 1-4.


#### 6.

```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false

from pyfixest.estimation import feols

def run_rd(df, cutoff, bandwidth):
    df = df.copy()
    
    # Define running variable
    df['running'] = df['raw_rating'] - (cutoff - 0.25)
    
    # Define treatment as exactly equal to cutoff star rating
    df['treat'] = (df['Star_Rating'] == cutoff).astype(int)
    
    # Filter only within bandwidth and to treated/control groups
    control_rating = cutoff - 0.5
    df_band = df[df['running'].between(-bandwidth, bandwidth) & df['Star_Rating'].isin([control_rating, cutoff])].copy()
    
    # Run RD regression
    model = feols("market_share ~ running + treat", data=df_band)
    
    return model

# Run RD at 3.0 and 3.5 cutoffs
model_3 = run_rd(df_2010, cutoff=3.0, bandwidth=0.125)
model_3_5 = run_rd(df_2010, cutoff=3.5, bandwidth=0.125)

# Extract relevant model info from pyfixest, now including Intercept
def extract_model_info(model, coef_labels):
    coef = model.coef()
    se = model.se()
    n = model._N
    r2 = model._r2

    # Format coef (se)
    formatted = {
        "Intercept": f"{coef.get('Intercept', float('nan')):.3f} ({se.get('Intercept', float('nan')):.3f})"
    }
    formatted.update({
        label: f"{coef.get(var, float('nan')):.3f} ({se.get(var, float('nan')):.3f})"
        for var, label in coef_labels.items()
    })
    formatted["N"] = n
    formatted["R2"] = round(r2, 3)
    return formatted

# Define label mappings matching new variable names
coef_labels = {
    "treat": "Rounded",
    "running": "Running Score"
}

# Extract model data
row_3 = extract_model_info(model_3, coef_labels)
row_35 = extract_model_info(model_3_5, coef_labels)

# Build final DataFrame including intercept
final_table = pd.DataFrame({
    "": ["Intercept", "Rounded", "Running Score", "N", "R2"],
    "3-Star Threshold": [
        row_3["Intercept"], row_3["Rounded"], row_3["Running Score"],
        row_3["N"], row_3["R2"]
    ],
    "3.5-Star Threshold": [
        row_35["Intercept"], row_35["Rounded"], row_35["Running Score"],
        row_35["N"], row_35["R2"]
    ]
})

# Set index column
final_table.set_index("", inplace=True)

# Display styled table
final_table.style.set_caption("RD Estimates at 3-Star and 3.5-Star Thresholds")

```

At the 3-star threshold, rounding increases market share by 2.4 percentage points, while at the 3.5-star threshold, the effect is slightly larger at 3.3 percentage points. The running score is also positively associated with market share at both thresholds, suggesting that even small differences in raw rating influence enrollment.

It is worth noting, however, that the R^2^ for both thresholds is very small.
{{< pagebreak >}}

#### 7. 
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
# Run RD at 3.0 and 3.5 cutoffs with different bandwidths
model_3_01 = run_rd(df_2010, cutoff=3.0, bandwidth=0.1)
model_3_012 = run_rd(df_2010, cutoff=3.0, bandwidth=0.12)
model_3_013 = run_rd(df_2010, cutoff=3.0, bandwidth=0.13)
model_3_014 = run_rd(df_2010, cutoff=3.0, bandwidth=0.14)
model_3_015 = run_rd(df_2010, cutoff=3.0, bandwidth=0.15)

model_35_01 = run_rd(df_2010, cutoff=3.5, bandwidth=0.1)
model_35_012 = run_rd(df_2010, cutoff=3.5, bandwidth=0.12)
model_35_013 = run_rd(df_2010, cutoff=3.5, bandwidth=0.13)
model_35_014 = run_rd(df_2010, cutoff=3.5, bandwidth=0.14)
model_35_015 = run_rd(df_2010, cutoff=3.5, bandwidth=0.15)


# Store all models with identifiers
models = {
    (3.0, 0.10): model_3_01,
    (3.0, 0.12): model_3_012,
    (3.0, 0.13): model_3_013,
    (3.0, 0.14): model_3_014,
    (3.0, 0.15): model_3_015,
    (3.5, 0.10): model_35_01,
    (3.5, 0.12): model_35_012,
    (3.5, 0.13): model_35_013,
    (3.5, 0.14): model_35_014,
    (3.5, 0.15): model_35_015,
}

# Collect results
rows = []
for (cutoff, bw), model in models.items():
    coef = model.coef()
    se = model.se()
    pval = model.pvalue()
    rows.append({
        'Cutoff': cutoff,
        'Bandwidth': bw,
        'Estimate': coef['treat'],
        'Std. Error': se['treat'],
        'p-Value': pval['treat']
    })

# Create DataFrame
results_df = pd.DataFrame(rows)

# Pivot into multi-level column table (like a grouped summary table)
pivot_df = pd.pivot_table(
    results_df,
    index='Bandwidth',
    columns='Cutoff',
    values=['Estimate', 'Std. Error', 'p-Value']
)

# Optional: Clean column names for readability
pivot_df.columns.names = ['Metric', 'Cutoff']
pivot_df

# Filter for 3.0 and 3.5 star thresholds
df_3 = results_df[results_df['Cutoff'] == 3.0]
df_35 = results_df[results_df['Cutoff'] == 3.5]

# Plot setup
plt.figure(figsize=(10, 6))

# Plot 3-star results
plt.errorbar(df_3['Bandwidth'], df_3['Estimate'], yerr=1.96 * df_3['Std. Error'],
             fmt='o', label='3-star', color='red', capsize=3)

# Plot 3.5-star results
plt.errorbar(df_35['Bandwidth'], df_35['Estimate'], yerr=1.96 * df_35['Std. Error'],
             fmt='x', label='3.5-star', color='green', capsize=3)

# Add plot elements
plt.axhline(0, color='gray', linestyle='--', linewidth=1)
plt.title('Treatment Effect by Bandwidth')
plt.xlabel('Bandwidth')
plt.ylabel('Estimated Treatment Effect')
plt.legend(title='Threshold')
plt.grid(True, linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

```

* For the 3.0-star threshold, the treatment effect estimates remain stable and significant across all bandwidths, indicating low sensitivity to bandwidth choice. The narrow and overlapping confidence intervals reinforce the robustness of the estimated effect of rounding to 3 stars.

* For the 3.5-star threshold, the treatment effect estimates fluctuate more noticeably across bandwidths, suggesting greater sensitivity. The wider confidence intervals, especially at smaller bandwidths, highlight less precision and more uncertainty in the effect of rounding to 3.5 stars.

#### 8.
```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
plt.figure(figsize=(12, 5))

# First plot: distribution around 3.0
plt.subplot(1, 2, 1)
sns.histplot(df_2010['raw_rating'], bins=50, kde=False, color='skyblue')
plt.axvline(3.0, color='red', linestyle='--', label='3.0 Cutoff')
plt.title('Distribution of Raw Ratings around 3.0')
plt.xlabel('Raw Rating')
plt.ylabel('Number of Plans')
plt.xlim(2.5, 3.5)
plt.legend()

# Second plot: distribution around 3.5
plt.subplot(1, 2, 2)
sns.histplot(df_2010['raw_rating'], bins=50, kde=False, color='lightgreen')
plt.axvline(3.5, color='red', linestyle='--', label='3.5 Cutoff')
plt.title('Distribution of Raw Ratings around 3.5')
plt.xlabel('Raw Rating')
plt.ylabel('Number of Plans')
plt.xlim(3.0, 4.0)
plt.legend()

plt.tight_layout()
plt.show()
```
These two histograms show the distribution of raw star ratings around the 3.0 and 3.5 cutoff points.

* At the 3.0 threshold, the distribution of the running variable shows potential signs of manipulation or rating bunching, which may challenge the validity of RD estimates there.
* In contrast, the 3.5 threshold appears more stable, suggesting it may be a more reliable setting for RD analysis.

##### Left Plot: 3.0 Cutoff
* There is a clear jump in density just below 3.0, especially around 2.7–2.9, with much higher counts of plans compared to the bins just above 3.0.
* The drop in frequency immediately after the cutoff suggests that fewer plans narrowly achieved a rating just above 3.0.
* This could indicate manipulation or bunching below the cutoff, where plans either failed to make it past the threshold or were rated conservatively.

##### Right Plot: 3.5 Cutoff
* The distribution appears smoother and more balanced around the 3.5 threshold.
* There's no obvious discontinuity or bunching just above or below 3.5 —> the frequency of plans seems fairly even across the window.
* This supports the RD assumption of continuity at 3.5, making it a more credible cutoff for causal inference.
{{< pagebreak >}}

#### 9.

```{python}
#| echo: false         
#| output: true      
#| warning: false     
#| message: false
# Step 1: Add binary indicators
df_2010['is_hmo'] = df_2010['plan_type'].str.contains('HMO', case=False, na=False).astype(int)
df_2010['has_partd'] = (df_2010['partd'] == 'Yes').astype(int)

# Step 2: Revised function to check covariate balance using STAR RATING (not raw_rating)
def covariate_balance_by_star(df, cutoff, bandwidth=0.125):
    df = df.copy()
    running = df['raw_rating'] - (cutoff - 0.25)
    
    # Keep within bandwidth and plans with exact Star_Rating values
    df_band = df[
        (running.between(-bandwidth, bandwidth)) &
        (df['Star_Rating'].isin([cutoff - 0.5, cutoff]))
    ].copy()
    
    df_band['above_cutoff'] = (df_band['Star_Rating'] == cutoff).astype(int)
    
    balance = df_band.groupby('above_cutoff')[['is_hmo', 'has_partd']].mean().T
    balance.columns = ['Below Cutoff', 'Above Cutoff']
    balance['Difference'] = balance['Above Cutoff'] - balance['Below Cutoff']
    return balance

# Step 3: Get covariate balance for 3.0 and 3.5 thresholds
balance_3 = covariate_balance_by_star(df_2010, cutoff=3.0)
balance_35 = covariate_balance_by_star(df_2010, cutoff=3.5)

# Step 4: Combine and label
balance_3['Cutoff'] = '3.0'
balance_35['Cutoff'] = '3.5'
balance_combined = pd.concat([balance_3, balance_35])
balance_combined.reset_index(inplace=True)
balance_combined.rename(columns={'index': 'Characteristic'}, inplace=True)

# Step 5: Round and return
balance_combined.round(3)
import seaborn as sns
import matplotlib.pyplot as plt

# Round for clarity and copy for plotting
plot_df = balance_combined.round(3).copy()

# Filter for each characteristic
hmo_df = plot_df[plot_df['Characteristic'] == 'is_hmo'].copy()
partd_df = plot_df[plot_df['Characteristic'] == 'has_partd'].copy()

# Melt both for plotting
hmo_melt = hmo_df.melt(id_vars=['Cutoff'], value_vars=['Below Cutoff', 'Above Cutoff'],
                       var_name='Group', value_name='Proportion')
partd_melt = partd_df.melt(id_vars=['Cutoff'], value_vars=['Below Cutoff', 'Above Cutoff'],
                           var_name='Group', value_name='Proportion')

# HMO Plot
plt.figure(figsize=(8, 5))
sns.barplot(data=hmo_melt, x='Cutoff', y='Proportion', hue='Group', palette='Blues', ci=None)
plt.title('HMO Plan Proportions Above vs. Below Star Cutoffs')
plt.ylim(0, 1)
plt.ylabel('Proportion of Plans that are HMO')
plt.xlabel('Star Rating Cutoff')
plt.grid(True, linestyle='--', alpha=0.3)
plt.legend(title='Group')
plt.tight_layout()
plt.show()

# Part D Plot
plt.figure(figsize=(8, 5))
sns.barplot(data=partd_melt, x='Cutoff', y='Proportion', hue='Group', palette='Greens', ci=None)
plt.title('Part D Coverage Proportions Above vs. Below Star Cutoffs')
plt.ylim(0, 1)
plt.ylabel('Proportion of Plans with Part D')
plt.xlabel('Star Rating Cutoff')
plt.grid(True, linestyle='--', alpha=0.3)
plt.legend(title='Group')
plt.tight_layout()
plt.show()

```

For HMO status, there is a significant jump at the 3.5 threshold: the proportion of HMO plans rises from just over 20% below the cutoff to over 50% above it, indicating a shift in plan type composition. The same can also be said around the 3.0 rating. According to the data, there are no HMO plans below the 3.0 rating. This could be either due to errors in the dataset, or that all plans with an HMO are automatically rated 3.0 stars and above. 

For Part D coverage, the difference is most significant at the 3.0 threshold. Plans just above the 3.0 cutoff are far more likely to offer drug coverage than those just below (around 87% vs. 52%). At the 3.5 threshold, Part D coverage remains high on both sides with minimal difference, suggesting little change in this characteristic across the higher threshold.

#### 10. Analysis of Points 5 - 9

Increasing a star rating has a positive and significant effect on Medicare Advantage plan enrollments. At the 3.0-star threshold, rounding up increases market share by 2.4 percentage points, and at 3.5 stars, the effect rises slightly to 3.3 points. The estimated treatment effects at 3.0 are stable and statistically precise across bandwidths, while those at 3.5 are more variable and less precise, suggesting the 3.0 threshold offers more robust evidence. However, bunching in raw ratings just below the 3.0 threshold raises concerns about potential manipulation, whereas the 3.5 threshold shows a smoother, more credible distribution. Overall, higher ratings attract more enrollees, but the reliability of RD estimates depends on the threshold examined.